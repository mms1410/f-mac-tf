import tensorflow as tf


# Full implementation of the dynamic algorithm with support for splitting gradients across GPUs
class HInvFastUpMulti:

    def __init__(self, grads, dev, gpus, damp=1e-5):
        self.m, self.d = grads.shape
        self.dev = dev
        self.gpus = gpus
        self.dtype = grads.dtype
        self.lambd = 1. / damp

        self.dper = self.d // len(gpus) + 1
        self.grads = []  # matrix $G$ in the paper
        for i in range(len(gpus)):
            start, end = i * self.dper, (i + 1) * self.dper
            self.grads.append(tf.Variable(grads[:, start:end], trainable=False))
        self.dots = tf.Variable(tf.zeros((self.m, self.m), dtype=self.dtype), trainable=False)  # matrix $GG^T$
        for i in range(len(gpus)):
            self.dots.assign_add(self.grads[i] @ tf.transpose(self.grads[i]))

        self.last = 0  # ringbuffer index
        self.giHig = self.lambd * self.dots  # matrix $D$
        self.denom = tf.Variable(tf.zeros(self.m, dtype=self.dtype), trainable=False)  # $D_ii + m$
        self.coef = tf.Variable(self.lambd * tf.eye(self.m, dtype=self.dtype), trainable=False)  # matrix $B$
        self.setup()

    def setup(self):
        self.giHig.assign(self.lambd * self.dots)
        diag = tf.linalg.diag(tf.fill([self.m], self.m, name=None))
        self.giHig = tf.linalg.lu(self.giHig + diag)[0]
        self.giHig = tf.linalg.band_part(self.giHig - diag, 0, -1)
        self.denom.assign(self.m + tf.linalg.tensor_diag_part(self.giHig))
        tmp = -tf.transpose(self.giHig) / tf.reshape(self.denom, (1, -1))

        for i in range(max(self.last, 1), self.m):
            self.coef[i, :i].assign(tmp[i, :i] @ self.coef[:i, :i])

    def grads_matmul(self, x):
        results = []

        def f(i):
            start, end = i * self.dper, (i + 1) * self.dper
            G = self.grads[i]
            return G @ x[start:end]

        for i in range(len(self.gpus)):
            results.append(f(i))
        return tf.reduce_sum(results, axis=0)

    def matmul_grads(self, x):
        results = []

        def f(G):
            return x @ G

        for G in self.grads:
            results.append(f(G))
        return tf.concat(results, axis=0)

    def set_grad(self, j, g):
        def f(i):
            start, end = i * self.dper, (i + 1) * self.dper
            self.grads[i][j, :].assign(g[start:end])

        for i in range(len(self.grads)):
            f(i)

    def mul(self, x, dots=None):
        if dots is None:
            dots = self.grads_matmul(x)
        giHix = self.lambd * dots
        for i in range(1, self.m):
            giHix[i:].assign_sub(self.giHig[i - 1, i:] * giHix[i - 1] / self.denom[i - 1])
        return self.lambd * x - self.matmul_grads((giHix / self.denom) @ self.coef)

    def update(self, g):
        self.set_grad(self.last, g)
        tmp = self.grads_matmul(g)
        self.dots[self.last, :].assign(tmp)
        self.dots[:, self.last].assign(tmp)
        self.setup()
        self.last = (self.last + 1) % self.m

    def update_mul(self, g):
        self.set_grad(self.last, g)
        tmp = self.grads_matmul(g)
        self.dots[self.last, :].assign(tmp)
        self.dots[:, self.last].assign(tmp)
        self.setup()
        res = self.mul(g, tmp)
        self.last = (self.last + 1) % self.m
        return res


if __name__ == '__main__':
    D = 1000
    M = 32
    dev = '/GPU:0'

    def dist(x, y):
        return tf.reduce_mean(tf.abs(x - y))

    grads = tf.random.normal((M, D), dtype=tf.float64)
    g = tf.random.normal((D,), dtype=tf.float64)
    hinv1 = HInvFastUpMulti(tf.zeros((M, D), dtype=tf.float64), dev, [dev])

    for i in range(M):
        hinv1.update(grads[i, :])
    print(dist(hinv1.mul(g), hinv1.mul(g)))

class MFAC(tf.optimizers.Optimizer):

    def __init__(self, lr=1e-3, momentum=0, weight_decay=0, ngrads=1024, damp=1e-5, moddev=None, optdev=None, gpus=[], sparse=False, name="MFAC", **kwargs):
        super(MFAC, self).__init__(name, **kwargs)
        self.momentum = momentum
        self.weight_decay = weight_decay
        self.moddev = moddev
        self.optdev = optdev
        self.sparse = sparse

        self.lr = lr

        w = []
        for var in self._var_list:
            w.append(tf.reshape(var, [-1]))
        w = tf.concat(w, axis=0)
        w = tf.identity(w)

        self.nweights = tf.size(w)

        if self.sparse:
            self.mask = w != 0
            w = tf.boolean_mask(w, self.mask)

        if self.momentum > 0:
            self.v = tf.zeros_like(w)

        if len(gpus) == 0:
            gpus = [self.optdev]

        self.hinv = HInvFastUpMulti(
            tf.zeros((ngrads, tf.size(w)), dtype=tf.float32), dev=self.optdev, gpus=gpus, damp=damp
        )

    @tf.function
    def _resource_apply_dense(self, grad, var):
        g = []

        if self.weight_decay > 0:
            tmp = grad + self.weight_decay * var
        else:
            tmp = grad
        g.append(tf.reshape(tmp, [-1]))

        g = tf.concat(g, axis=0)

        if self.sparse:
            g = tf.boolean_mask(g, self.mask)

        tmp = self.hinv.update_mul(g)

        if self.momentum > 0:
            self.v.assign(self.momentum * self.v + (1 - self.momentum) * tmp)
            tmp = self.v

        if self.sparse:
            expanded = tf.zeros(self.nweights, dtype=var.dtype)
            indices = tf.where(self.mask)
            tmp = tf.scatter_nd(indices, tmp, expanded.shape)
        
        count = 0
        for var in self._var_list:
            var.assign_add(tmp[count:(count + tf.size(var))], alpha=-self.lr)
            count += tf.size(var)
        
    def _resource_apply_sparse(self, grad, var):
        raise NotImplementedError("Sparse tensor updates are not supported.")

class HInvSlow:

    def __init__(self, grads, damp=1e-5):
        m, d = grads.shape
        H = tf.linalg.diag(tf.ones([d], dtype=grads.dtype) / damp)

        for i in range(m):
            g = grads[i, :]
            Hg = tf.linalg.matvec(H, g)
            outer_product = tf.linalg.outer(Hg, Hg)
            H -= outer_product / (m + tf.tensordot(g, Hg, axes=1))
            
        self.H = H

    def mul(self, x):
        return tf.linalg.matvec(self.H, x)

# Small test comparing dynamic algorithm results with naive Woodbury implementation
if __name__ == '__main__':
    D = 1000
    M = 32
    DEV = tf.device('/GPU:0')

    def dist(x, y):
        return tf.reduce_mean(tf.abs(x - y))

    with DEV:
        grads = tf.random.normal((M, D), dtype=tf.float64)
        g = tf.random.normal((D,), dtype=tf.float64)
        # Beachten Sie, dass HInvFastUpMulti auch in TensorFlow übersetzt werden muss,
        # bevor Sie diesen Code ausführen können.
        hinv1 = HInvFastUpMulti(tf.zeros((M, D), dtype=tf.float64), DEV, [DEV])
        hinv2 = HInvSlow(grads)

        for i in range(M):
            hinv1.update(grads[i, :])
        print(dist(hinv1.mul(g), hinv2.mul(g)))
